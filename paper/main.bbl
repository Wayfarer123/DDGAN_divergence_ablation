\begin{thebibliography}{10}

\bibitem{https://doi.org/10.48550/arxiv.1701.07875}
Martin Arjovsky, Soumith Chintala, and L?on Bottou.
\newblock Wasserstein gan, 2017.

\bibitem{https://doi.org/10.48550/arxiv.2105.05233}
Prafulla Dhariwal and Alex Nichol.
\newblock Diffusion models beat gans on image synthesis, 2021.

\bibitem{https://doi.org/10.48550/arxiv.1406.2661}
Ian~J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David
  Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial networks, 2014.

\bibitem{DBLP:journals/corr/GulrajaniAADC17}
Ishaan Gulrajani, Faruk Ahmed, Mart{\'{\i}}n Arjovsky, Vincent Dumoulin, and
  Aaron~C. Courville.
\newblock Improved training of wasserstein gans.
\newblock {\em CoRR}, abs/1704.00028, 2017.

\bibitem{https://doi.org/10.48550/arxiv.1706.08500}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp
  Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock 2017.

\bibitem{https://doi.org/10.48550/arxiv.2006.11239}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models, 2020.

\bibitem{https://doi.org/10.48550/arxiv.1812.04948}
Tero Karras, Samuli Laine, and Timo Aila.
\newblock A style-based generator architecture for generative adversarial
  networks, 2018.

\bibitem{https://doi.org/10.48550/arxiv.1312.6114}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes, 2013.

\bibitem{https://doi.org/10.48550/arxiv.2106.00132}
Zhifeng Kong and Wei Ping.
\newblock On fast sampling of diffusion probabilistic models, 2021.

\bibitem{1705001}
F.~Liese and I.~Vajda.
\newblock On divergences and informations in statistics and information theory.
\newblock {\em IEEE Transactions on Information Theory}, 52(10):4394--4412,
  2006.

\bibitem{Nguyen_2010}
XuanLong Nguyen, Martin~J. Wainwright, and Michael~I. Jordan.
\newblock Estimating divergence functionals and the likelihood ratio by convex
  risk minimization.
\newblock {\em {IEEE} Transactions on Information Theory}, 56(11):5847--5861,
  nov 2010.

\bibitem{https://doi.org/10.48550/arxiv.2102.09672}
Alex Nichol and Prafulla Dhariwal.
\newblock Improved denoising diffusion probabilistic models, 2021.

\bibitem{https://doi.org/10.48550/arxiv.1606.00709}
Sebastian Nowozin, Botond Cseke, and Ryota Tomioka.
\newblock f-gan: Training generative neural samplers using variational
  divergence minimization, 2016.

\bibitem{ronneberger2015unet}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation,
  2015.

\bibitem{https://doi.org/10.48550/arxiv.2010.08029}
Matt Shannon, Ben Poole, Soroosh Mariooryad, Tom Bagby, Eric Battenberg, David
  Kao, Daisy Stanton, and RJ~Skerry-Ryan.
\newblock Non-saturating gan training as divergence minimization, 2020.

\bibitem{https://doi.org/10.48550/arxiv.1503.03585}
Jascha Sohl-Dickstein, Eric~A. Weiss, Niru Maheswaranathan, and Surya Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics, 2015.

\bibitem{szegedy2015rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and
  Zbigniew Wojna.
\newblock Rethinking the inception architecture for computer vision, 2015.

\bibitem{xiao2017fashionmnist}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms, 2017.

\bibitem{https://doi.org/10.48550/arxiv.2112.07804}
Zhisheng Xiao, Karsten Kreis, and Arash Vahdat.
\newblock Tackling the generative learning trilemma with denoising diffusion
  gans, 2021.

\end{thebibliography}
